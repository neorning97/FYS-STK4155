{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb10d9d",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Exercise week 47\n",
    "Nadia Ørning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f3ae78",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 1: Linear and logistic regression methods\n",
    "\n",
    "1. What is the main difference between ordinary least squares and Ridge regression?\n",
    "- OLS aims to minimize the sum of the squared residuals. Ridge regression does the same, but it also has a penalty term (L2 norm of the coefficients).\n",
    "\n",
    "2. Which kind of data set would you use logistic regression for?\n",
    "- I would use logistic regression for binary classification problems where we want the output variable to be two outcomes (e.g. 0/1, yes/no)\n",
    "  \n",
    "3. In linear regression you assume that your output is described by a continuous non-stochastic function $f(x)$. Which is the equivalent function in logistic regression?\n",
    "- The equivalent function is the sigmoid function, which maps the input x to a probability value between 0 and 1.\n",
    "  \n",
    "4. Can you find an analytic solution to a logistic regression type of problem?\n",
    "- No, logistic regression models are estimated using iterative optimization methods.\n",
    "\n",
    "5. What kind of cost function would you use in logistic regression?\n",
    "- I would use the cross-entropy cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755cfd27",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 2: Deep learning\n",
    "\n",
    "1. What is an activation function and discuss the use of an activation function? Explain three different types of activation functions?\n",
    "- An activation function introduces non-linearity into a neural network. This allows it to learn and make decisions based on complex non-linear data patterns.\n",
    "    - Sigmoid function: maps input values to an output range between 0 and 1 using the sigmoid curve.\n",
    "    - ReLU: outputs the input directly if it is positive, else it outputs 0.\n",
    "    - Leaky ReLU: a variation of ReLU, but it allows a small non-zero gradient when the input is negative.\n",
    "      \n",
    "2. Describe the architecture of a typical feed forward  Neural Network (NN). \n",
    "- Consists of many layers:\n",
    "    - Input layer: receives the input data and passes it to the next layer. The number of neurons in this layer is the same as the number of features in the input data.\n",
    "    - Hidden layers: layers between input and output layers. Can have one or more hidden layers, with multiple neurons in each layer. Here, it applies weight, biases, and activation functions on the input data.\n",
    "    - Output layers: produced the final output. The number of neurons depends on the nature of the task.\n",
    "\n",
    "3. You are using a deep neural network for a prediction task. After training your model, you notice that it is strongly overfitting the training set and that the performance on the test isn’t good. What can you do to reduce overfitting?\n",
    "- I can use regularization techniques, like apply L1 or L2 regularization to penalize the weights.\n",
    "- I can increase the amount of training data.\n",
    "- I can also reduce the complexity (reduce number of layers or neurons per layer)\n",
    "\n",
    "4. How would you know if your model is suffering from the problem of exploding Gradients?\n",
    "-I would see NaN or the loss value oscillate a lot, or activations and weights become very big.\n",
    "\n",
    "5. Can you name and explain a few hyperparameters used for training a neural network?\n",
    "- learning rate: determines the step size of each iteration.\n",
    "- batch size: number of training examples used in one forward or backward pass.\n",
    "- regularization parameters: the strength of L1 or L2 applied to the weights. \n",
    "\n",
    "6. Describe the architecture of a typical Convolutional Neural Network (CNN)\n",
    "- It consists of convolution layers, activation functions, pooling layers, and fully connected layers.\n",
    "  \n",
    "7. What is the vanishing gradient problem in Neural Networks and how to fix it?\n",
    "- It happens when gradients become extremely small during backpropagation, which makes it hard for the network to update the weights and learn well. We can fix it by using activation functions.\n",
    "  \n",
    "8. When it comes to training an artificial neural network, what could the reason be for why the cost/loss doesn't decrease in a few epochs?\n",
    "- Why it doesn't increase could be because of: the learning rate is too high/low, the weight initialization is poor, not enough data preprocessing..\n",
    "  \n",
    "9. How does L1/L2 regularization affect a neural network?\n",
    "- L1 adds a penalty term similar to the absolute value of the weights. L2 adds a penalty term similar to the square of the weights.\n",
    "  \n",
    "10. What is(are) the advantage(s) of deep learning over traditional methods like linear regression or logistic regression?\n",
    "- we can model complex patterns, we can handle large-scale data .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85175b87",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 3: Decision trees and ensemble methods\n",
    "\n",
    "1. Mention some pros and cons when using decision trees\n",
    "- pro: easy to understand and interpret, and can handle both numerical and categorical data.\n",
    "- con: can easily overfit the training data, and can be biased.\n",
    "  \n",
    "2. How do we grow a tree? And which are the main parameters? \n",
    "- We first split the data into smaller groups step by step, and at each step a rule is chosen to separate the data into two parts. This continues until the group is small enough or meets a stopping rule.\n",
    "- Main parameters: max depth, min samples to split, min samples per leaf, max features.\n",
    "  \n",
    "3. Mention some of the benefits with using ensemble methods (like bagging, random forests and boosting methods)?\n",
    "- Some benefits are improved accuracy, and bias/variance reduction.\n",
    "  \n",
    "4. Why would you prefer a random forest instead of using Bagging to grow a forest?\n",
    "- Random forest also has feature randomness at each split, and it's less likely that overfitting happens.\n",
    "  \n",
    "5. What is the basic philosophy behind boosting methods?\n",
    "- Combine a sequence of weak learners to create a strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfdfe68",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 4: Optimization part\n",
    "\n",
    "1. Which is the basic mathematical root-finding method behind essentially all gradient descent approaches(stochastic and non-stochastic)?\n",
    "-  The basic mathematical root-finding method is Newton's method.\n",
    "  \n",
    "2. And why don't we use it? Or stated differently, why do we introduce the learning rate as a parameter?\n",
    "- Because it's expensive computationally. It needs the invertion of the Hessian matrix.\n",
    "  \n",
    "3. What might happen if you set the momentum hyperparameter too close to 1 (e.g., 0.9999) when using an optimizer for the learning rate?\n",
    "- it can cause overshooting and slow convergence.\n",
    "\n",
    "4. Why should we use stochastic gradient descent instead of plain gradient descent?\n",
    "- SGD is more efficient for large data. It can also get out of the local minima which GD can be in.\n",
    "  \n",
    "5. Which parameters would you need to tune when use a stochastic gradient descent approach?\n",
    "- learning rate, momentum, learning rate schedule, batch size, number of epochs, and the regularisation parameter lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fc1b0c",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Exercise 5: Analysis of results\n",
    "1. How do you assess overfitting and underfitting?\n",
    "- If you compare the test and training data errors, overfitting is when the error of training is decreased (highly accurate) while the error of the test is increased (poor performance).\n",
    "- Underfitting is when both test and training data is high (poor performance for both). (Hastie et al figure)\n",
    "\n",
    "2. Why do we divide the data in test and train and/or eventually validation sets?\n",
    "- this is to check how well the performance of the model on \"unseen\" data (test) is, and optimize the hyperparameters while preventing over- and underfitting (validation)\n",
    "\n",
    "3. Why would you use resampling methods in the data analysis? Mention some widely popular resampling methods.\n",
    "- I want to use resampling methods to better estimate the accuracy of the model, that is, to reduce overfitting and improve robustness.\n",
    "- Bootstrap and cross-validation (k-fold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
